{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n**Data Preprocessing and Fusion for Unsplash Image Dataset**\n\n**Introduction:**\nThis notebook presents a comprehensive data preprocessing and fusion workflow for the Unsplash Image Dataset. The dataset contains a wealth of information, including image attributes, keywords, conversions, colors, and collections. Our goal is to prepare and integrate this data to create a consolidated dataset for further analysis and modeling.\n\n**Data Preprocessing Steps:**\n\n1. **Data Import and Initial Filtering:**\n   - We begin by loading the dataset from the Unsplash Image Dataset, which includes images and their attributes.\n   - Initial filtering is performed to exclude rows with missing values in the 'ai_description' column.\n\n2. **Attribute Selection:**\n   - We select relevant attributes for our analysis, including image attributes, exif data, location information, and more.\n\n3. **Keyword Integration:**\n   - We merge the filtered dataset with a keywords dataset based on the 'photo_id' column.\n   - This step enhances the dataset with additional information related to keywords and confidence scores from AI services.\n\n4. **Data Type Conversion and Cleaning:**\n   - Data types are converted as necessary, and rows with invalid values (NaN) in specific columns are removed.\n   - Duplicate rows based on the 'photo_id' are also addressed.\n\n5. **Conversions Integration:**\n   - We merge the dataset with a conversions dataset, adding information related to conversion country and additional keywords.\n\n6. **Colors Integration:**\n   - The dataset is enriched with data from a colors dataset, including hex values, RGB values, keywords, AI coverage, and AI score.\n\n7. **Collections Integration:**\n   - We merge the dataset with collections data, introducing collection titles for each image.\n\n**Conclusion:**\nThis Kaggle notebook demonstrates a thorough data preprocessing and integration process for the Unsplash Image Dataset. By the end of this workflow, we have created a consolidated dataset that combines image attributes, keywords, conversions, colors, and collections. This dataset is now ready for advanced analysis, modeling, and insights generation.\n\n**Acknowledgments:**\nWe acknowledge the Unsplash dataset for providing a rich source of image-related information for this project.","metadata":{"_uuid":"bf2a929d-b0bd-4241-a3de-940a444a6dc6","_cell_guid":"6155b236-a4ba-4f4f-a0fc-e801c1c52fcf","trusted":true}},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for input and output (replace with your actual paths)\ninput_file_path = '/kaggle/input/unsplash-dataset-lite/photos.tsv000'\noutput_file_path = '/kaggle/working/filtered_dataset.csv'  # Include the desired file name\n\n# Load your dataset into a pandas DataFrame\ndf = pd.read_csv(input_file_path, sep='\\t')  # Specify the tab separator for TSV\n\n# Filter the DataFrame to keep rows where 'ai_description' is not null\ndf_filtered = df.dropna(subset=['ai_description'])\n\n# Save the filtered dataset to the specified output CSV file\ndf_filtered.to_csv(output_file_path, index=False)","metadata":{"_uuid":"efca8dff-55c5-4507-a340-c076025fdece","_cell_guid":"6fc5e0af-3c1a-4445-9f97-3209a7c6bb49","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T10:18:33.300872Z","iopub.execute_input":"2023-10-09T10:18:33.301708Z","iopub.status.idle":"2023-10-09T10:18:34.220154Z","shell.execute_reply.started":"2023-10-09T10:18:33.301669Z","shell.execute_reply":"2023-10-09T10:18:34.218788Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for input and output (replace with your actual paths)\ninput_file_path = '/kaggle/working/filtered_dataset.csv'\noutput_file_path = '/kaggle/working/new_filtered_dataset.csv'  # Include the desired file name\n\n# Load the dataset from the input file\ndf = pd.read_csv(input_file_path)\n\n# Define the list of relevant attributes to keep\nrelevant_attributes = [\n    'photo_id',\n    'photo_image_url',\n    'photo_width',\n    'photo_height',\n    'photo_aspect_ratio',\n    'photo_description',\n    'exif_camera_make',\n    'exif_camera_model',\n    'exif_iso',\n    'exif_aperture_value',\n    'exif_focal_length',\n    'exif_exposure_time',\n    'photo_location_name',\n    'photo_location_latitude',\n    'photo_location_longitude',\n    'photo_location_country',\n    'photo_location_city',\n    'stats_views',\n    'stats_downloads',\n    'ai_description',\n    'ai_primary_landmark_name',\n    'ai_primary_landmark_latitude',\n    'ai_primary_landmark_longitude',\n    'ai_primary_landmark_confidence',\n    'blur_hash'\n]\n\n# Create a new DataFrame with only the relevant attributes\ndf_filtered = df[relevant_attributes]\n\n# Save the filtered dataset to the specified output CSV file\ndf_filtered.to_csv(output_file_path, index=False)","metadata":{"_uuid":"4a559a3f-26b5-4b62-bef9-611fa8abf754","_cell_guid":"31bb34e7-acd8-492a-856f-014418352773","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T10:29:57.787461Z","iopub.execute_input":"2023-10-09T10:29:57.788017Z","iopub.status.idle":"2023-10-09T10:29:58.476279Z","shell.execute_reply.started":"2023-10-09T10:29:57.787978Z","shell.execute_reply":"2023-10-09T10:29:58.475447Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for the filtered dataset and keywords dataset\nfiltered_dataset_path = '/kaggle/working/new_filtered_dataset.csv'\nkeywords_dataset_path = '/kaggle/input/unsplash-dataset-lite/keywords.tsv000'\n\n# Load the filtered dataset\ndf_filtered = pd.read_csv(filtered_dataset_path)\n\n# Load the keywords dataset with tab separation\ndf_keywords = pd.read_csv(keywords_dataset_path, sep='\\t')\n\n# Merge the two datasets based on the 'photo_id' column\nmerged_df = pd.merge(df_filtered, df_keywords[['photo_id', 'keyword', 'ai_service_1_confidence', 'ai_service_2_confidence']], on='photo_id', how='left')\n\n# Save the merged dataset to a new CSV file\noutput_file_path = '/kaggle/working/merged_dataset.csv'  # Specify the desired output file path\nmerged_df.to_csv(output_file_path, index=False)","metadata":{"_uuid":"0a69e082-c598-4266-8fe7-c78ec1e01f83","_cell_guid":"ffbd3ae4-885f-439e-b299-79c5233f539c","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T10:44:39.113974Z","iopub.execute_input":"2023-10-09T10:44:39.114359Z","iopub.status.idle":"2023-10-09T10:45:22.816470Z","shell.execute_reply.started":"2023-10-09T10:44:39.114332Z","shell.execute_reply":"2023-10-09T10:45:22.815493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file path for the merged dataset\nmerged_dataset_path = '/kaggle/working/merged_dataset.csv'\n\n# Load the merged dataset with specified data types and low_memory=False\ndf_merged = pd.read_csv(merged_dataset_path, low_memory=False)\n\n# Define a function to handle the conversion of 'exif_iso' column values\ndef convert_exif_iso(value):\n    try:\n        return float(value)\n    except (ValueError, TypeError):\n        return None\n\n# Apply the custom conversion function to the 'exif_iso' column\ndf_merged['exif_iso'] = df_merged['exif_iso'].apply(convert_exif_iso)\n\n# Remove rows with invalid values (NaN) in the 'exif_iso' and 'exif_aperture_value' columns\ndf_merged = df_merged.dropna(subset=['exif_iso', 'exif_aperture_value'])\n\n# Remove duplicate rows based on the 'photo_id' column\ndf_merged_no_duplicates = df_merged.drop_duplicates(subset='photo_id', keep='first')\n\n# Save the dataset with duplicate rows removed to a new CSV file\noutput_file_path = '/kaggle/working/merged_dataset_no_duplicates_1.csv'  # Specify the desired output file path\ndf_merged_no_duplicates.to_csv(output_file_path, index=False)","metadata":{"_uuid":"f5110876-4b85-43d0-8cd8-38214153cd43","_cell_guid":"05c57dcd-8165-4bfe-9acc-75b086102ca7","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T10:58:51.623369Z","iopub.execute_input":"2023-10-09T10:58:51.623832Z","iopub.status.idle":"2023-10-09T10:59:08.309022Z","shell.execute_reply.started":"2023-10-09T10:58:51.623790Z","shell.execute_reply":"2023-10-09T10:59:08.308048Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for the datasets\nconversions_file_path = '/kaggle/input/unsplash-dataset-lite/conversions.tsv000'\nexisting_dataset_path = '/kaggle/working/merged_dataset_no_duplicates_1.csv'\noutput_file_path = '/kaggle/working/merged_dataset_with_conversions.csv'\n\n# Load the conversions dataset\nconversions_df = pd.read_csv(conversions_file_path, sep='\\t')  # Assuming it's a TSV file\n\n# Load the existing dataset\nexisting_df = pd.read_csv(existing_dataset_path)\n\n# Merge the datasets using the 'photo_id' column\nmerged_df = existing_df.merge(conversions_df[['photo_id', 'conversion_country', 'keyword']], on='photo_id', how='left')\n\n# Handle duplicates (if necessary) by dropping them based on the 'photo_id'\nmerged_df_no_duplicates = merged_df.drop_duplicates(subset='photo_id', keep='first')\n\n# Save the merged dataset to a new CSV file\nmerged_df_no_duplicates.to_csv(output_file_path, index=False)","metadata":{"_uuid":"d61ee812-dce4-4e2e-a753-3eef9f066f0c","_cell_guid":"3e81241e-9ac3-4c20-afc4-bd3903638f5a","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T11:04:45.865150Z","iopub.execute_input":"2023-10-09T11:04:45.865544Z","iopub.status.idle":"2023-10-09T11:05:27.964823Z","shell.execute_reply.started":"2023-10-09T11:04:45.865515Z","shell.execute_reply":"2023-10-09T11:05:27.963295Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for the datasets\ncolors_file_path = '/kaggle/input/unsplash-dataset-lite/colors.tsv000'\nexisting_dataset_path = '/kaggle/working/merged_dataset_with_conversions.csv'\noutput_file_path = '/kaggle/working/merged_dataset_with_colors.csv'\n\n# Load the colors dataset\ncolors_df = pd.read_csv(colors_file_path, sep='\\t')  # Assuming it's a TSV file\n\n# Load the existing dataset\nexisting_df = pd.read_csv(existing_dataset_path)\n\n# Merge the datasets using the 'photo_id' column\nmerged_df = existing_df.merge(\n    colors_df[['photo_id', 'hex', 'red', 'green', 'blue', 'keyword', 'ai_coverage', 'ai_score']],\n    on='photo_id',\n    how='left'\n)\n\n# Handle duplicates (if necessary) by dropping them based on the 'photo_id'\nmerged_df_no_duplicates = merged_df.drop_duplicates(subset='photo_id', keep='first')\n\n# Save the merged dataset to a new CSV file\nmerged_df_no_duplicates.to_csv(output_file_path, index=False)","metadata":{"_uuid":"4f21dc06-8bea-47d6-b32d-f6612042fd0e","_cell_guid":"b7e1ae32-219f-4698-9eba-f14607b6d817","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T11:10:09.332258Z","iopub.execute_input":"2023-10-09T11:10:09.334157Z","iopub.status.idle":"2023-10-09T11:10:11.110812Z","shell.execute_reply.started":"2023-10-09T11:10:09.334097Z","shell.execute_reply":"2023-10-09T11:10:11.109493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for the datasets\ncollections_file_path = '/kaggle/input/unsplash-dataset-lite/collections.tsv000'\nexisting_dataset_path = '/kaggle/working/merged_dataset_with_colors.csv'\noutput_file_path = '/kaggle/working/merged_dataset_with_collections.csv'\n\n# Load the collections dataset\ncollections_df = pd.read_csv(collections_file_path, sep='\\t')  # Assuming it's a TSV file\n\n# Load the existing dataset\nexisting_df = pd.read_csv(existing_dataset_path)\n\n# Merge the datasets using the 'photo_id' column\nmerged_df = existing_df.merge(\n    collections_df[['photo_id', 'collection_title']],\n    on='photo_id',\n    how='left'\n)\n\n# Handle duplicates (if necessary) by dropping them based on the 'photo_id'\nmerged_df_no_duplicates = merged_df.drop_duplicates(subset='photo_id', keep='first')\n\n# Save the merged dataset to a new CSV file\nmerged_df_no_duplicates.to_csv(output_file_path, index=False)","metadata":{"_uuid":"adb761a4-83cd-483a-a1d4-315eea2a34f5","_cell_guid":"0535557b-8e08-4073-8814-9bd66fee31e2","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T11:14:48.705167Z","iopub.execute_input":"2023-10-09T11:14:48.705682Z","iopub.status.idle":"2023-10-09T11:14:57.996760Z","shell.execute_reply.started":"2023-10-09T11:14:48.705628Z","shell.execute_reply":"2023-10-09T11:14:57.995462Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths for the existing dataset and the output file\nexisting_dataset_path = '/kaggle/working/merged_dataset_with_collections.csv'\noutput_file_path = '/kaggle/working/dataset_final.csv'\n\n# Load the existing dataset\nexisting_df = pd.read_csv(existing_dataset_path)\n\n# List of columns to remove\ncolumns_to_remove = [\n    'photo_location_name',\n    'photo_location_latitude',\n    'photo_location_longitude',\n    'photo_location_country',\n    'photo_location_city'\n]\n\n# Drop the specified columns from the dataset\nfiltered_df = existing_df.drop(columns=columns_to_remove)\n\n# Handle duplicates (if necessary) by dropping them based on the 'photo_id'\nfiltered_df_no_duplicates = filtered_df.drop_duplicates(subset='photo_id', keep='first')\n\n# Save the filtered dataset to a new CSV file\nfiltered_df_no_duplicates.to_csv(output_file_path, index=False)","metadata":{"_uuid":"c3cc2de6-e366-41e2-89f1-8baa305099e1","_cell_guid":"69a9c3e9-42d2-4036-ada2-d67dc89a93b8","collapsed":false,"execution":{"iopub.status.busy":"2023-10-09T11:18:09.824416Z","iopub.execute_input":"2023-10-09T11:18:09.824982Z","iopub.status.idle":"2023-10-09T11:18:10.507163Z","shell.execute_reply.started":"2023-10-09T11:18:09.824945Z","shell.execute_reply":"2023-10-09T11:18:10.506112Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}